from sklearn.feature_extraction.text import CountVectorizer
import seaborn as sns
import streamlit as st
import pandas as pd
from konlpy.tag import Mecab
import matplotlib.pyplot as plt

def nouns_count1(data,cat,limit = 0) :
    m = Mecab()
    stopwords = ['ê²ƒ','ì €','ìˆ˜','ì—”','ê°€','ì´','ì€','ë„','ê±°ë“ ìš”','ì–´','ìœ¼ë¡œ',
    'ì—','ì´ë‘','ë‹ˆ','ë„','ë˜','ë‹ˆê¹Œ','ì„','ê±°','ëŒ€ì²´','ê²Œ','ê¹¨','ë•Œ','ë‚˜','ë²ˆ','ì œ','ë°','ì• ','ì „','ë‚´','ì „','ê±´','ë­”ê°€','ë‚ ',
    'í•˜ë£¨','ëˆ„ê°€','ë­','ì ','ì§€','ê±¸','ë•Œë¬¸','ê·¸ê²Œ','ì´ê²Œ','ì •ë„','í›„','ë­˜','ì¤‘','ì´ê²ƒ','ê·¸ê±°','ì´ê±°','ê±´ê°€','ê¹€','ê±¸ê¹Œ',
    'ê±°ê¸°','ê±”','ì–¼ë§ˆ','ê·¸ê±¸','ì–´ë””','ì¤„','ë‚œ','ê±¸ê¹Œìš”','ë„ˆ','ë­”ì§€','ë­˜ê¹Œìš”','ë•','ê°€ìš”','ì¯¤','ë»”','ìŒ','ë§‰']
    if cat == 'ì „ì²´' :
        context = data['context'].to_list()
    else :
        context = data[data['category_under_1'] == cat]['context'].to_list()
    n_nouns = []

    for t in context :
        for n in m.nouns(t) : # ëª…ì‚¬ ì¶”ì¶œ
            if len(n) > limit :
                if n not in stopwords : 
                    n_nouns.append(n)
        
    # count = Counter(n_nouns)
    # n_best = count.most_common(100) # ë¹ˆë„ ìˆœ ì¶”ì¶œ
    
    # x,y = [], []
    
    # for word, count in n_best :
    #     x.append(word)
    #     y.append(count)

    return n_nouns

def corr_map(data, max_features=500):  # max features = 1000 ë“± ë†’ì€ê°’ ì—ëŸ¬ ì´ìŠˆ renameí•˜ì§€ ì•Šì€ì½”ë“œëŠ” ì •ìƒì‘ë™
        cv = CountVectorizer(max_features=max_features)
        tdm = cv.fit_transform(data['ë‹¨ì–´'])
        df = pd.DataFrame(data=tdm.toarray(), columns=cv.get_feature_names_out(), index=list(data.category.unique()))
        df = df.transpose()

        sns.clustermap(df.corr(),
                       annot=True,
                       cmap='RdYlBu_r',
                       vmin=-1, vmax=1,
                       figsize=(12, 10))



def app() :
    st.title('ì¹´í…Œê³ ë¦¬ë³„ ìƒê´€ê´€ê³„ ë¶„ì„')
    st.write('ğŸ“Œ ê° ì¹´í…Œê³ ë¦¬ ì „ì²´ì— ëŒ€í•´ì„œ count ê¸°ë°˜ ìƒìœ„ 500 ë‹¨ì–´ì— ëŒ€í•œ ìƒê´€ê³„ìˆ˜ì™€ ê·¸ì— ëŒ€í•œ ë´ë“œë¡œê·¸ë¨ ì…ë‹ˆë‹¤.')
    st.write('ğŸ“Œ ë‹¨ì–´ ë§¤í•‘ì— ëŒ€í•œ ìƒê´€ê´€ê³„ì´ê¸°ì— ê°’ì´ í´ìˆ˜ë¡ ê°™ì€ ë‹¨ì–´ì˜ ì‚¬ìš© íšŸìˆ˜ ì°¨ì´ê°€ ë¯¸ë¹„í•©ë‹ˆë‹¤. ì¦‰, ê°™ì€ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•œ íšŸìˆ˜ê°€ ë¹„ìŠ·í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')
    url1 = '/opt/ml/streamlit/eda/data/answer.csv'
    url2 = '/opt/ml/streamlit/eda/data/question.csv'
    data = pd.read_csv(url1)
    data1 = pd.read_csv(url2)
    data.drop(['Unnamed: 0'],axis = 1, inplace = True)
    data1.drop(['Unnamed: 0'],axis = 1, inplace = True)
    copy_df = data.copy()
    copy_df1 = data1.copy()
    copy_df['category_under_2']= copy_df['category'].apply(lambda x : '/'.join(x.split('/')[:2]))
    copy_df['category_under_1']= copy_df['category'].apply(lambda x : '/'.join(x.split('/')[:1]))

    copy_df1['category_under_2']= copy_df1['category'].apply(lambda x : '/'.join(x.split('/')[:2]))
    copy_df1['category_under_1']= copy_df1['category'].apply(lambda x : '/'.join(x.split('/')[:1]))

    word_dict = {}
    for i in copy_df1.category_under_1.unique() :
        n_best = nouns_count1(copy_df1,i)
        word_dict[i] = n_best

    df2 = pd.DataFrame(columns = ['ë‹¨ì–´','category'])
    idx1 = 0
    for i,v in word_dict.items() :
        df2.loc[idx1,'category'] = i
        df2['ë‹¨ì–´'][idx1] = ' '.join(v)
        idx1 += 1
    
    corr_map(df2)

    st.pyplot(plt)